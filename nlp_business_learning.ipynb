{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b955e0b",
   "metadata": {},
   "source": [
    "# NLP for Business â€” Reproducible Learning Journey\n",
    "\n",
    "## Overview\n",
    "This notebook documents hands-on NLP workflows for analyzing text using:\n",
    "- **Preprocessing** (cleaning, stopwords, regex)\n",
    "- **Frequency analysis** (before/after cleaning)\n",
    "- **Topic modeling (LDA)** (hyperparameter tuning, coherence evaluation)\n",
    "- **Visualization** (pyLDAvis interactive exploration)\n",
    "\n",
    "**Dataset:** 20 Newsgroups (a classic NLP corpus with 20 categories)\n",
    "\n",
    "**Key Learning:** *\"Clean text determines meaningful topics.\"* Most NLP performance gains come from ruthless text cleaning, not fancy algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Modules Covered\n",
    "1. **Module 1.2** â€” Data Preparation & Source Consistency\n",
    "2. **Module 1.3** â€” How LDA Topic Modeling Works\n",
    "3. **Module 1.4** â€” Stopwords & Frequency Analysis\n",
    "4. **Module 1.5** â€” Topic Modeling Experiments\n",
    "5. **Module 2.1** â€” Regex Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13edf9",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import all necessary packages for NLP analysis, topic modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# NLP and text processing\n",
    "import tomotopy as tp\n",
    "import gensim\n",
    "import gensim.corpora\n",
    "import gensim.models.coherencemodel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"Tomotopy version: {tp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b2362",
   "metadata": {},
   "source": [
    "## Section 2: Load and Explore Dataset\n",
    "\n",
    "**Module 1.2 Lesson:** *Source consistency matters before modeling*\n",
    "\n",
    "We'll use the 20 Newsgroups dataset as our corpus. This has 20 different categories of documents - similar to how companies span different industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45817b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 20 Newsgroups dataset (removing headers, footers, quotes for cleaner text)\n",
    "print(\"Loading 20 Newsgroups dataset...\")\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups.data,\n",
    "    'category': [newsgroups.target_names[i] for i in newsgroups.target]\n",
    "})\n",
    "\n",
    "print(f\"âœ“ Loaded {len(df)} documents\")\n",
    "print(f\"âœ“ {len(newsgroups.target_names)} categories\\n\")\n",
    "print(\"Categories:\", newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploration\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(df.info())\n",
    "print(\"\\n=== SAMPLE DOCUMENTS ===\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Add word count\n",
    "df['nWords'] = df['text'].str.split().apply(len)\n",
    "\n",
    "print(\"\\n=== DOCUMENT LENGTH STATISTICS ===\")\n",
    "print(df['nWords'].describe())\n",
    "\n",
    "# Visualize length distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['nWords'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Document Length Distribution')\n",
    "plt.axvline(df['nWords'].median(), color='red', linestyle='--', label=f'Median: {df[\"nWords\"].median():.0f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "category_counts = df['category'].value_counts()\n",
    "plt.barh(category_counts.head(10).index, category_counts.head(10).values)\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.title('Top 10 Categories by Document Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdca83a",
   "metadata": {},
   "source": [
    "## Section 3: Raw Word Frequency Analysis\n",
    "\n",
    "**Module 1.4 Lesson:** *Raw frequencies reflect language, not meaning*\n",
    "\n",
    "Before any cleaning, let's see what words dominate the corpus. We expect to see mostly grammar words like \"the\", \"and\", \"of\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all documents (raw, no preprocessing)\n",
    "all_words_raw = []\n",
    "for text in df['text']:\n",
    "    words = str(text).lower().split()\n",
    "    all_words_raw.extend(words)\n",
    "\n",
    "# Count frequencies\n",
    "word_freq_raw = Counter(all_words_raw)\n",
    "top_20_raw = word_freq_raw.most_common(20)\n",
    "\n",
    "print(\"=== TOP 20 MOST COMMON WORDS (RAW TEXT) ===\")\n",
    "for word, count in top_20_raw:\n",
    "    print(f\"{word:20s} {count:,}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "words, counts = zip(*top_20_raw)\n",
    "plt.barh(range(len(words)), counts, color='steelblue')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 Words (Raw Text) - Dominated by Grammar Words', fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Notice: The top words are meaningless grammar words!\")\n",
    "print(\"   These words tell us nothing about document content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d504af",
   "metadata": {},
   "source": [
    "## Section 4: Text Preprocessing Pipeline\n",
    "\n",
    "**Key Insight:** *Preprocessing dominates results*\n",
    "\n",
    "We'll implement a multi-stage cleaning pipeline:\n",
    "1. Lowercase conversion\n",
    "2. Remove URLs and email addresses\n",
    "3. Remove numbers\n",
    "4. Remove special characters\n",
    "5. Remove stopwords (generic + custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59768e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=False, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "        remove_stopwords: Whether to remove stopwords\n",
    "        custom_stopwords: Additional stopwords to remove\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove numbers (Module 2.1 lesson)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters and extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords if requested\n",
    "    if remove_stopwords:\n",
    "        words = text.split()\n",
    "        \n",
    "        # Start with sklearn's ENGLISH_STOP_WORDS\n",
    "        stopwords = set(ENGLISH_STOP_WORDS)\n",
    "        \n",
    "        # Add custom stopwords if provided\n",
    "        if custom_stopwords:\n",
    "            stopwords.update(custom_stopwords)\n",
    "        \n",
    "        # Filter out stopwords and very short words\n",
    "        words = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the function on a sample document\n",
    "sample_idx = 0\n",
    "sample_text = df.loc[sample_idx, 'text']\n",
    "\n",
    "print(\"=== BEFORE CLEANING ===\")\n",
    "print(sample_text[:500])\n",
    "print(\"\\n=== AFTER CLEANING (no stopword removal) ===\")\n",
    "print(clean_text(sample_text, remove_stopwords=False)[:500])\n",
    "print(\"\\n=== AFTER CLEANING (with stopword removal) ===\")\n",
    "print(clean_text(sample_text, remove_stopwords=True)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7202d",
   "metadata": {},
   "source": [
    "## Section 5: Stopword Removal & Custom Filtering\n",
    "\n",
    "**Module 1.4 Lesson:** *Removing stopwords shifts signals from grammar to content*\n",
    "\n",
    "Generic stopwords aren't enough. We need **domain-specific** stopwords too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7baaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning with stopword removal\n",
    "df['cleanText'] = df['text'].apply(lambda x: clean_text(x, remove_stopwords=True))\n",
    "\n",
    "# Add word count after cleaning\n",
    "df['nWords_clean'] = df['cleanText'].str.split().apply(len)\n",
    "\n",
    "# Filter out very short documents\n",
    "df = df[df['nWords_clean'] > 10].reset_index(drop=True)\n",
    "\n",
    "print(f\"After filtering: {len(df)} documents remaining\")\n",
    "print(f\"Average words per document (cleaned): {df['nWords_clean'].mean():.1f}\")\n",
    "\n",
    "# Recompute word frequencies on cleaned text\n",
    "all_words_clean = []\n",
    "for text in df['cleanText']:\n",
    "    words = text.split()\n",
    "    all_words_clean.extend(words)\n",
    "\n",
    "word_freq_clean = Counter(all_words_clean)\n",
    "top_20_clean = word_freq_clean.most_common(20)\n",
    "\n",
    "print(\"\\n=== TOP 20 MOST COMMON WORDS (CLEANED TEXT) ===\")\n",
    "for word, count in top_20_clean:\n",
    "    print(f\"{word:20s} {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f58174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw vs cleaned frequencies side-by-side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw frequencies\n",
    "words_raw, counts_raw = zip(*top_20_raw)\n",
    "ax1.barh(range(len(words_raw)), counts_raw, color='lightcoral')\n",
    "ax1.set_yticks(range(len(words_raw)))\n",
    "ax1.set_yticklabels(words_raw)\n",
    "ax1.set_xlabel('Frequency')\n",
    "ax1.set_title('Top 20 Words - RAW TEXT\\n(Grammar words dominate)', fontweight='bold', fontsize=12)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Cleaned frequencies\n",
    "words_clean, counts_clean = zip(*top_20_clean)\n",
    "ax2.barh(range(len(words_clean)), counts_clean, color='lightgreen')\n",
    "ax2.set_yticks(range(len(words_clean)))\n",
    "ax2.set_yticklabels(words_clean)\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.set_title('Top 20 Words - CLEANED TEXT\\n(Content words emerge)', fontweight='bold', fontsize=12)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ KEY OBSERVATION:\")\n",
    "print(\"   After cleaning, meaningful content words surface!\")\n",
    "print(\"   Words like 'people', 'think', 'know', 'just' are more informative.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65435757",
   "metadata": {},
   "source": [
    "## Section 6: Helper Functions for LDA\n",
    "\n",
    "**Module 1.3 Lesson:** *LDA discovers latent themes via probabilistic word co-occurrence*\n",
    "\n",
    "LDA treats:\n",
    "- Each **document** as a mix of topics\n",
    "- Each **topic** as a mix of words\n",
    "\n",
    "We need functions to:\n",
    "1. Build LDA models\n",
    "2. Calculate coherence (how interpretable are the topics?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCoherence(model, ncore=4):\n",
    "    \"\"\"\n",
    "    Calculate coherence score for a tomotopy LDA model\n",
    "    \n",
    "    Higher coherence = more interpretable topics\n",
    "    \"\"\"\n",
    "    topics = []\n",
    "    for k in range(model.k):\n",
    "        word_probs = model.get_topic_words(k)\n",
    "        topics.append([word for word, prob in word_probs])\n",
    "        \n",
    "    texts = []\n",
    "    corpus = []\n",
    "    for doc in model.docs:\n",
    "        words = [model.vocabs[token_id] for token_id in doc.words]\n",
    "        texts.append(words)\n",
    "        freqs = list(collections.Counter(doc.words).items())\n",
    "        corpus.append(freqs)\n",
    "    \n",
    "    id2word = dict(enumerate(model.vocabs))\n",
    "    dictionary = gensim.corpora.dictionary.Dictionary.from_corpus(\n",
    "        corpus, id2word\n",
    "    )\n",
    "    \n",
    "    cm = gensim.models.coherencemodel.CoherenceModel(\n",
    "        topics=topics,\n",
    "        texts=texts,\n",
    "        corpus=corpus,\n",
    "        dictionary=dictionary,\n",
    "        processes=ncore\n",
    "    )\n",
    "    \n",
    "    return cm.get_coherence()\n",
    "\n",
    "\n",
    "def buildModel(df, hP, txt_col, k, alpha, eta):\n",
    "    \"\"\"\n",
    "    Build and train an LDA model using tomotopy\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with documents\n",
    "        hP: Hyperparameters dict\n",
    "        txt_col: Column name with text\n",
    "        k: Number of topics\n",
    "        alpha: Document-topic concentration (higher = more topics per doc)\n",
    "        eta: Topic-word concentration (higher = more words per topic)\n",
    "    \n",
    "    Returns:\n",
    "        Trained LDA model\n",
    "    \"\"\"\n",
    "    model = tp.LDAModel(\n",
    "        alpha=alpha,      \n",
    "        eta=eta,   \n",
    "        k=k\n",
    "    )\n",
    "\n",
    "    df_split = [doc.split() for doc in df[txt_col].astype(str)]\n",
    "    for doc in df_split: \n",
    "        model.add_doc(doc)\n",
    "\n",
    "    model.train(iter=hP['train_iter'])\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3e655",
   "metadata": {},
   "source": [
    "## Section 7: Hyperparameter Tuning - Number of Topics (k)\n",
    "\n",
    "**Goal:** Find the optimal number of topics\n",
    "\n",
    "**Metrics:**\n",
    "- **Perplexity:** Lower is better (how well model predicts held-out data)\n",
    "- **Coherence:** Higher is better (how interpretable are topics)\n",
    "\n",
    "We'll test k = [10, 15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f27bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hP = {\n",
    "    'train_iter': 500,  # Number of training iterations\n",
    "    'n_cores': 4,       # Parallel processing\n",
    "}\n",
    "\n",
    "# Test different numbers of topics\n",
    "coherence_k = []\n",
    "perplexity_k = []\n",
    "ks = [10, 15, 20, 25, 30]\n",
    "alphas = [0.1]\n",
    "etas = [0.01]\n",
    "\n",
    "print(\"=== TUNING NUMBER OF TOPICS (k) ===\\n\")\n",
    "for k in ks:\n",
    "    for alpha in alphas:\n",
    "        for eta in etas:\n",
    "            print(f\"Training: k={k}, alpha={alpha}, eta={eta}...\", end=' ')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            ldaModel = buildModel(df, hP, 'cleanText', k, alpha, eta)\n",
    "            p = ldaModel.perplexity\n",
    "            c = getCoherence(ldaModel, 4)\n",
    "            \n",
    "            coherence_k.append([k, alpha, eta, c])\n",
    "            perplexity_k.append([k, alpha, eta, p])\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Done in {elapsed:.1f}s | Perplexity: {p:.4f}, Coherence: {c:.4f}\")\n",
    "\n",
    "# Find best k\n",
    "best_k_idx = np.argmax([x[3] for x in coherence_k])\n",
    "best_k = coherence_k[best_k_idx][0]\n",
    "print(f\"\\nâœ“ Best k (by coherence): {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed605e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for k tuning\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "X = [x[0] for x in coherence_k]\n",
    "Y = [y[3] for y in coherence_k]\n",
    "ax1.plot(X, Y, marker='o', linewidth=2, markersize=8, color='green')\n",
    "ax1.set_title('Topic Coherence by Number of Topics', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Number of Topics (k)', fontsize=11)\n",
    "ax1.set_ylabel('Coherence Score (higher = better)', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axvline(best_k, color='red', linestyle='--', alpha=0.7, label=f'Best k={best_k}')\n",
    "ax1.legend()\n",
    "\n",
    "X = [x[0] for x in perplexity_k]\n",
    "Y = [y[3] for y in perplexity_k]\n",
    "ax2.plot(X, Y, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_title('Model Perplexity by Number of Topics', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Number of Topics (k)', fontsize=11)\n",
    "ax2.set_ylabel('Perplexity (lower = better)', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ INTERPRETATION:\")\n",
    "print(\"   - Coherence peaks suggest optimal topic interpretability\")\n",
    "print(\"   - Perplexity generally decreases with more topics\")\n",
    "print(\"   - Balance: Choose k with high coherence, reasonable perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495b620",
   "metadata": {},
   "source": [
    "## Section 8: Hyperparameter Tuning - Alpha\n",
    "\n",
    "**Alpha:** Document-topic distribution parameter\n",
    "- **Lower alpha** â†’ Documents focus on fewer topics\n",
    "- **Higher alpha** â†’ Documents spread across many topics\n",
    "\n",
    "We'll test alpha = [0.01, 0.1, 0.5, 1.0, 2.0] with the best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_alphas = []\n",
    "perplexity_alphas = []\n",
    "ks = [best_k]\n",
    "alphas = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "etas = [0.01]\n",
    "\n",
    "print(\"=== TUNING ALPHA ===\\n\")\n",
    "for k in ks:\n",
    "    for alpha in alphas:\n",
    "        for eta in etas:\n",
    "            print(f\"Training: k={k}, alpha={alpha}, eta={eta}...\", end=' ')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            ldaModel = buildModel(df, hP, 'cleanText', k, alpha, eta)\n",
    "            p = ldaModel.perplexity\n",
    "            c = getCoherence(ldaModel, 4)\n",
    "            \n",
    "            coherence_alphas.append([k, alpha, eta, c])\n",
    "            perplexity_alphas.append([k, alpha, eta, p])\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Done in {elapsed:.1f}s | Perplexity: {p:.4f}, Coherence: {c:.4f}\")\n",
    "\n",
    "# Find best alpha\n",
    "best_alpha_idx = np.argmax([x[3] for x in coherence_alphas])\n",
    "best_alpha = coherence_alphas[best_alpha_idx][1]\n",
    "print(f\"\\nâœ“ Best alpha (by coherence): {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alpha tuning results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "X = [x[1] for x in coherence_alphas]\n",
    "Y = [y[3] for y in coherence_alphas]\n",
    "ax1.plot(X, Y, marker='o', linewidth=2, markersize=8, color='green')\n",
    "ax1.set_title('Topic Coherence by Alpha', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Alpha (document-topic concentration)', fontsize=11)\n",
    "ax1.set_ylabel('Coherence Score', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axvline(best_alpha, color='red', linestyle='--', alpha=0.7, label=f'Best Î±={best_alpha}')\n",
    "ax1.legend()\n",
    "\n",
    "X = [x[1] for x in perplexity_alphas]\n",
    "Y = [y[3] for y in perplexity_alphas]\n",
    "ax2.plot(X, Y, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_title('Model Perplexity by Alpha', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Alpha (document-topic concentration)', fontsize=11)\n",
    "ax2.set_ylabel('Perplexity', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bf474",
   "metadata": {},
   "source": [
    "## Section 9: Hyperparameter Tuning - Eta\n",
    "\n",
    "**Eta:** Topic-word distribution parameter\n",
    "- **Lower eta** â†’ Topics focus on fewer words (more specific)\n",
    "- **Higher eta** â†’ Topics spread across many words (more general)\n",
    "\n",
    "We'll test eta = [0.001, 0.01, 0.05, 0.1, 0.2] with best k and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_etas = []\n",
    "perplexity_etas = []\n",
    "ks = [best_k]\n",
    "alphas = [best_alpha]\n",
    "etas = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "\n",
    "print(\"=== TUNING ETA ===\\n\")\n",
    "for k in ks:\n",
    "    for alpha in alphas:\n",
    "        for eta in etas:\n",
    "            print(f\"Training: k={k}, alpha={alpha}, eta={eta}...\", end=' ')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            ldaModel = buildModel(df, hP, 'cleanText', k, alpha, eta)\n",
    "            p = ldaModel.perplexity\n",
    "            c = getCoherence(ldaModel, 4)\n",
    "            \n",
    "            coherence_etas.append([k, alpha, eta, c])\n",
    "            perplexity_etas.append([k, alpha, eta, p])\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Done in {elapsed:.1f}s | Perplexity: {p:.4f}, Coherence: {c:.4f}\")\n",
    "\n",
    "# Find best eta\n",
    "best_eta_idx = np.argmax([x[3] for x in coherence_etas])\n",
    "best_eta = coherence_etas[best_eta_idx][2]\n",
    "print(f\"\\nâœ“ Best eta (by coherence): {best_eta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot eta tuning results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "X = [x[2] for x in coherence_etas]\n",
    "Y = [y[3] for y in coherence_etas]\n",
    "ax1.plot(X, Y, marker='o', linewidth=2, markersize=8, color='green')\n",
    "ax1.set_title('Topic Coherence by Eta', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Eta (topic-word concentration)', fontsize=11)\n",
    "ax1.set_ylabel('Coherence Score', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axvline(best_eta, color='red', linestyle='--', alpha=0.7, label=f'Best Î·={best_eta}')\n",
    "ax1.legend()\n",
    "\n",
    "X = [x[2] for x in perplexity_etas]\n",
    "Y = [y[3] for y in perplexity_etas]\n",
    "ax2.plot(X, Y, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_title('Model Perplexity by Eta', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Eta (topic-word concentration)', fontsize=11)\n",
    "ax2.set_ylabel('Perplexity', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… OPTIMAL HYPERPARAMETERS FOUND:\")\n",
    "print(f\"   k (topics): {best_k}\")\n",
    "print(f\"   alpha: {best_alpha}\")\n",
    "print(f\"   eta: {best_eta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1c3c2",
   "metadata": {},
   "source": [
    "## Section 10: Build Final Model with Optimal Parameters\n",
    "\n",
    "Now we'll train a final model with more iterations using our optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aefd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== BUILDING FINAL MODEL ===\")\n",
    "print(f\"Hyperparameters: k={best_k}, alpha={best_alpha}, eta={best_eta}\")\n",
    "print(\"Training with 1000 iterations...\\n\")\n",
    "\n",
    "# Build final model with more training iterations\n",
    "hP_final = {\n",
    "    'train_iter': 1000,\n",
    "    'n_cores': 4,\n",
    "    'eta': best_eta,\n",
    "    'alpha': best_alpha,\n",
    "    'k': best_k\n",
    "}\n",
    "\n",
    "def buildFinalModel(df, hP, txt_col):\n",
    "    \"\"\"Build final model using parameters from hP dict\"\"\"\n",
    "    model = tp.LDAModel(\n",
    "        alpha=hP['alpha'],      \n",
    "        eta=hP['eta'],   \n",
    "        k=hP['k']\n",
    "    )\n",
    "\n",
    "    df_split = [doc.split() for doc in df[txt_col].astype(str)]\n",
    "    for doc in df_split: \n",
    "        model.add_doc(doc)\n",
    "\n",
    "    model.train(iter=hP['train_iter'])\n",
    "\n",
    "    return model\n",
    "\n",
    "start_time = time.time()\n",
    "final_model = buildFinalModel(df, hP_final, 'cleanText')\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ Training complete in {elapsed:.1f}s\")\n",
    "print(f\"\\n### MODEL EVALUATION ###\")\n",
    "print(f\"Perplexity: {final_model.perplexity:.4f}\")\n",
    "print(f\"Coherence: {getCoherence(final_model, 4):.4f}\")\n",
    "print(f\"Total vocabulary size: {len(final_model.vocabs)}\")\n",
    "print(f\"Number of documents: {len(final_model.docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82aec5",
   "metadata": {},
   "source": [
    "## Section 11: Explore Topics\n",
    "\n",
    "**Module 1.5 Lesson:** *Coherent topics emerge from strong co-occurrence patterns*\n",
    "\n",
    "Let's examine what topics our model discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0090a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### TOP WORDS PER TOPIC ###\\n\")\n",
    "for k in range(final_model.k):\n",
    "    words = final_model.get_topic_words(k, top_n=10)\n",
    "    word_list = ', '.join([word for word, prob in words])\n",
    "    print(f'Topic #{k}: {word_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and data\n",
    "print(\"\\n=== SAVING MODEL ===\")\n",
    "final_model.save(\"newsgroups_lda_final.mdl\")\n",
    "pickle.dump(df, open(\"newsgroups_df.p\", \"wb\"))\n",
    "print(\"âœ“ Model saved as 'newsgroups_lda_final.mdl'\")\n",
    "print(\"âœ“ DataFrame saved as 'newsgroups_df.p'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db7433",
   "metadata": {},
   "source": [
    "## Section 12: Interactive Visualization with pyLDAvis\n",
    "\n",
    "**pyLDAvis** creates an interactive HTML visualization showing:\n",
    "- Topic clusters in 2D space\n",
    "- Topic prevalence (size)\n",
    "- Top terms per topic\n",
    "- Term frequency distributions\n",
    "\n",
    "This is the most intuitive way to explore and interpret LDA results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24595745",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CREATING PYLDAVIS VISUALIZATION ===\")\n",
    "\n",
    "# Prepare data for pyLDAvis\n",
    "topic_term_dists = np.stack([final_model.get_topic_word_dist(k) for k in range(final_model.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in final_model.docs])\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "doc_lengths = np.array([len(doc.words) for doc in final_model.docs])\n",
    "vocab = list(final_model.used_vocabs)\n",
    "term_frequency = final_model.used_vocab_freq\n",
    "\n",
    "print(\"Preparing interactive visualization...\")\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency,\n",
    "    start_index=0,  # tomotopy uses 0-based indexing\n",
    "    sort_topics=False  # Keep topic IDs consistent\n",
    ")\n",
    "\n",
    "# Save as HTML\n",
    "pyLDAvis.save_html(prepared_data, 'newsgroups_ldavis.html')\n",
    "print(\"âœ“ Interactive visualization saved as 'newsgroups_ldavis.html'\")\n",
    "print(\"\\nðŸ’¡ Open 'newsgroups_ldavis.html' in your browser to explore topics interactively!\")\n",
    "\n",
    "# Display in notebook (if running in Jupyter)\n",
    "# pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e448e",
   "metadata": {},
   "source": [
    "## Section 13: LDA Validation and Analysis\n",
    "\n",
    "Beyond visual inspection, we need **quantitative validation** of our topics. This section implements systematic checks to validate topic quality and interpretability.\n",
    "\n",
    "### What We'll Check:\n",
    "1. **Term-Topic Assignment** - Which topic maximizes each term?\n",
    "2. **Topic Separation** - Are topics distinct or overlapping?\n",
    "3. **Term Weights Analysis** - Compare related terms across topics\n",
    "4. **Model Metrics** - Final perplexity and coherence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f7408",
   "metadata": {},
   "source": [
    "### 13.1: Find Which Topic Maximizes a Term\n",
    "\n",
    "For any word, we want to know: **Which topic has the highest P(word | topic)?**\n",
    "\n",
    "This helps identify topic content without relying on visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_term_topic(model, term, top_n=500):\n",
    "    \"\"\"\n",
    "    Find the topic where a term has the highest probability P(word | topic)\n",
    "    \n",
    "    Args:\n",
    "        model: Trained tomotopy LDA model\n",
    "        term: Word to search for\n",
    "        top_n: Number of top words to check per topic\n",
    "    \n",
    "    Returns:\n",
    "        (best_topic_id, best_probability)\n",
    "    \"\"\"\n",
    "    best_topic = None\n",
    "    best_prob = 0.0\n",
    "    \n",
    "    for k in range(model.k):\n",
    "        words = dict(model.get_topic_words(k, top_n=top_n))\n",
    "        if term in words and words[term] > best_prob:\n",
    "            best_prob = words[term]\n",
    "            best_topic = k\n",
    "    \n",
    "    return best_topic, best_prob\n",
    "\n",
    "# Test with some example terms from the dataset\n",
    "test_terms = [\"computer\", \"game\", \"space\", \"religion\", \"medical\", \"car\", \"politics\"]\n",
    "\n",
    "print(\"=== TERM-TOPIC ASSIGNMENTS ===\\n\")\n",
    "for term in test_terms:\n",
    "    topic_id, prob = find_term_topic(final_model, term)\n",
    "    if topic_id is not None:\n",
    "        print(f\"{term:15s} -> Topic {topic_id:2d} (P={prob:.5f})\")\n",
    "        # Show top 5 words for that topic\n",
    "        top_words = ', '.join([w for w, p in final_model.get_topic_words(topic_id, top_n=5)])\n",
    "        print(f\"                Top words: {top_words}\\n\")\n",
    "    else:\n",
    "        print(f\"{term:15s} -> Not found in top {500} words\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada3830",
   "metadata": {},
   "source": [
    "### 13.2: Compare Related Terms Across Topics\n",
    "\n",
    "For semantically related terms, we want to see which topics emphasize which words. This reveals topic specialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f535d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_weights_by_topic(model, terms, topn=500):\n",
    "    \"\"\"\n",
    "    Get P(word | topic) for multiple terms across all topics\n",
    "    \n",
    "    Args:\n",
    "        model: Trained tomotopy LDA model\n",
    "        terms: List of terms to analyze\n",
    "        topn: Number of top words to check per topic\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping topic_id -> {term: probability}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k in range(model.k):\n",
    "        w = dict(model.get_topic_words(k, top_n=topn))\n",
    "        hits = {t: w.get(t, 0.0) for t in terms if t in w}\n",
    "        if hits:\n",
    "            out[k] = hits\n",
    "    return out\n",
    "\n",
    "# Example: Analyze technology-related terms\n",
    "tech_terms = [\"computer\", \"software\", \"hardware\", \"programming\", \"algorithm\", \"data\"]\n",
    "\n",
    "print(\"=== TERM WEIGHTS ACROSS TOPICS (Technology Terms) ===\\n\")\n",
    "tw = term_weights_by_topic(final_model, tech_terms)\n",
    "\n",
    "for term in tech_terms:\n",
    "    # Rank topics by this term's probability\n",
    "    ranked = sorted(\n",
    "        [(k, tw[k].get(term, 0.0)) for k in tw if term in tw[k]],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    if ranked:\n",
    "        print(f\"\\n{term}:\")\n",
    "        for k, val in ranked[:3]:  # Top 3 topics\n",
    "            print(f\"  Topic {k:2d}: {val:.5f}\")\n",
    "            \n",
    "# Show detailed view of the top topic for \"computer\"\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED VIEW: Topic containing 'computer'\")\n",
    "print(\"=\"*60)\n",
    "comp_topic, _ = find_term_topic(final_model, \"computer\")\n",
    "if comp_topic is not None:\n",
    "    print(f\"\\nTopic {comp_topic} - Top 15 words:\")\n",
    "    for word, prob in final_model.get_topic_words(comp_topic, top_n=15):\n",
    "        print(f\"  {word:20s} {prob:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ef2c9",
   "metadata": {},
   "source": [
    "### 13.3: Topic Separation Metric\n",
    "\n",
    "**Quantify topic overlap** using cosine similarity between topic-word distributions.\n",
    "\n",
    "- **Lower average similarity** = more distinct topics\n",
    "- **Higher average similarity** = overlapping/redundant topics\n",
    "\n",
    "**Rule of thumb:**\n",
    "- < 0.3: Topics are highly distinct\n",
    "- 0.3-0.5: Moderate separation\n",
    "- > 0.5: Topics are overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a657de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get topic-word distributions for all topics\n",
    "topic_dists = np.stack([\n",
    "    final_model.get_topic_word_dist(k)\n",
    "    for k in range(final_model.k)\n",
    "])\n",
    "\n",
    "# Calculate cosine similarity between all topic pairs\n",
    "sim_matrix = cosine_similarity(topic_dists)\n",
    "\n",
    "# Calculate average off-diagonal similarity\n",
    "k = final_model.k\n",
    "avg_sim = (sim_matrix.sum() - k) / (k * (k - 1))\n",
    "\n",
    "print(\"=== TOPIC SEPARATION ANALYSIS ===\\n\")\n",
    "print(f\"Number of topics: {k}\")\n",
    "print(f\"Average topic cosine similarity: {avg_sim:.4f}\")\n",
    "\n",
    "if avg_sim < 0.3:\n",
    "    print(\"âœ“ EXCELLENT: Topics are highly distinct\")\n",
    "elif avg_sim < 0.5:\n",
    "    print(\"âœ“ GOOD: Topics have moderate separation\")\n",
    "else:\n",
    "    print(\"âš  WARNING: Topics may be overlapping\")\n",
    "\n",
    "# Find most similar topic pairs\n",
    "print(\"\\n=== MOST SIMILAR TOPIC PAIRS ===\")\n",
    "similar_pairs = []\n",
    "for i in range(k):\n",
    "    for j in range(i+1, k):\n",
    "        similar_pairs.append((i, j, sim_matrix[i, j]))\n",
    "\n",
    "similar_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\nTop 5 most similar topic pairs:\")\n",
    "for i, j, sim in similar_pairs[:5]:\n",
    "    print(f\"\\nTopics {i} & {j} (similarity: {sim:.4f})\")\n",
    "    print(f\"  Topic {i}: {', '.join([w for w, p in final_model.get_topic_words(i, top_n=5)])}\")\n",
    "    print(f\"  Topic {j}: {', '.join([w for w, p in final_model.get_topic_words(j, top_n=5)])}\")\n",
    "\n",
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(sim_matrix, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Topic Similarity Matrix\\n(Darker = More Similar)', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Topic ID', fontsize=12)\n",
    "plt.ylabel('Topic ID', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cddef7",
   "metadata": {},
   "source": [
    "### 13.4: Final Model Quality Metrics\n",
    "\n",
    "Summary of all key metrics for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL QUALITY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š MODEL CONFIGURATION\")\n",
    "print(f\"  Number of topics (k):        {best_k}\")\n",
    "print(f\"  Alpha (doc-topic):           {best_alpha}\")\n",
    "print(f\"  Eta (topic-word):            {best_eta}\")\n",
    "print(f\"  Training iterations:         {hP_final['train_iter']}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ PERFORMANCE METRICS\")\n",
    "final_perplexity = final_model.perplexity\n",
    "final_coherence = getCoherence(final_model, 4)\n",
    "print(f\"  Perplexity (lower=better):   {final_perplexity:.4f}\")\n",
    "print(f\"  Coherence (higher=better):   {final_coherence:.4f}\")\n",
    "print(f\"  Topic separation (avg sim):  {avg_sim:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“š CORPUS STATISTICS\")\n",
    "print(f\"  Documents processed:         {len(final_model.docs):,}\")\n",
    "print(f\"  Vocabulary size:             {len(final_model.vocabs):,}\")\n",
    "print(f\"  Used vocabulary:             {len(final_model.used_vocabs):,}\")\n",
    "\n",
    "print(\"\\nâœ… QUALITY ASSESSMENT\")\n",
    "if final_coherence > 0.4:\n",
    "    print(\"  âœ“ Coherence: EXCELLENT\")\n",
    "elif final_coherence > 0.3:\n",
    "    print(\"  âœ“ Coherence: GOOD\")\n",
    "else:\n",
    "    print(\"  âš  Coherence: NEEDS IMPROVEMENT\")\n",
    "\n",
    "if avg_sim < 0.3:\n",
    "    print(\"  âœ“ Topic Separation: EXCELLENT\")\n",
    "elif avg_sim < 0.5:\n",
    "    print(\"  âœ“ Topic Separation: GOOD\")\n",
    "else:\n",
    "    print(\"  âš  Topic Separation: NEEDS IMPROVEMENT\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ’¡ INTERPRETATION TIPS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"  â€¢ Use pyLDAvis HTML to explore topics interactively\")\n",
    "print(\"  â€¢ Check term-topic assignments to understand topic content\")\n",
    "print(\"  â€¢ Review topic separation matrix for redundant topics\")\n",
    "print(\"  â€¢ Consider retraining with different k if topics overlap heavily\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef70dd",
   "metadata": {},
   "source": [
    "### 13.5: Common Pitfalls to Avoid\n",
    "\n",
    "âš ï¸ **Important Reminders:**\n",
    "\n",
    "1. **Don't identify topics by pyLDAvis bubble position alone**\n",
    "   - Always click the bubble and read the term bars on the right\n",
    "   - Or use the `find_term_topic()` function to identify topics programmatically\n",
    "\n",
    "2. **Ensure your final model uses the parameters you claim**\n",
    "   - Check that k, alpha, eta match what you report\n",
    "   - Verify training iterations were actually applied\n",
    "\n",
    "3. **Burn-in doesn't apply automatically**\n",
    "   - If you set `burn_in` in a config dict, you must apply it:\n",
    "   ```python\n",
    "   model.burn_in = hP[\"burn_in\"]\n",
    "   ```\n",
    "\n",
    "4. **High coherence alone isn't enough**\n",
    "   - Also check topic separation\n",
    "   - Review actual term-topic assignments\n",
    "   - Validate with domain knowledge\n",
    "\n",
    "5. **Document your evidence**\n",
    "   - Save term-topic maxima for key terms\n",
    "   - Record topic separation metrics\n",
    "   - Export and version control your pyLDAvis HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8e065",
   "metadata": {},
   "source": [
    "## Section 14: Key Takeaways & Lessons Learned\n",
    "\n",
    "### Core Lessons from This Analysis\n",
    "\n",
    "1. **Preprocessing dominates results**\n",
    "   - Most performance gains came from text cleaning, not modeling\n",
    "   - Stopword removal is critical\n",
    "   - Domain-specific filtering matters\n",
    "\n",
    "2. **Generic language is the enemy**\n",
    "   - Grammar words hide meaningful content\n",
    "   - Custom stopwords are essential\n",
    "   - Regex cleaning removes noise (numbers, dates)\n",
    "\n",
    "3. **Hyperparameter tuning is worth it**\n",
    "   - Different k values produce very different topics\n",
    "   - Alpha and eta fine-tune topic granularity\n",
    "   - Coherence helps find optimal parameters\n",
    "\n",
    "4. **Context matters**\n",
    "   - More text per document â†’ better topics\n",
    "   - Short descriptions struggle to show clear themes\n",
    "   - Combining sources improves interpretability\n",
    "\n",
    "5. **Domain knowledge beats generic NLP**\n",
    "   - Understanding your data helps filter noise\n",
    "   - Custom preprocessing pipelines are necessary\n",
    "   - One-size-fits-all solutions don't work\n",
    "\n",
    "6. **Validation is essential**\n",
    "   - Don't rely on visualizations alone\n",
    "   - Use quantitative metrics (coherence, separation)\n",
    "   - Check term-topic assignments programmatically\n",
    "   - Document your evidence\n",
    "\n",
    "---\n",
    "\n",
    "### The NLP Pipeline for Text Analysis\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "  â†“\n",
    "Source Standardization\n",
    "  â†“\n",
    "Lowercasing\n",
    "  â†“\n",
    "Remove URLs, emails, special chars\n",
    "  â†“\n",
    "Remove numbers (if appropriate)\n",
    "  â†“\n",
    "Stopword Removal (generic + domain-specific)\n",
    "  â†“\n",
    "Frequency Analysis\n",
    "  â†“\n",
    "Topic Modeling (LDA)\n",
    "  â†“\n",
    "Hyperparameter Tuning\n",
    "  â†“\n",
    "Evaluation & Interpretation\n",
    "  â†“\n",
    "Visualization (pyLDAvis)\n",
    "  â†“\n",
    "Validation (term-topic, separation metrics)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Final Insight\n",
    "\n",
    "> **\"Topic modeling on text is less about fancy algorithms and more about ruthless text cleaning. The model only reflects whatever language survives preprocessing.\"**\n",
    "\n",
    "> **\"Visual inspection is not enough. Always validate with quantitative metrics and term-topic assignments.\"**\n",
    "\n",
    "**Clean text + rigorous validation = meaningful topics.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
